{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocessing as proc\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor \n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-ticks')\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.labelsize'] = 22\n",
    "plt.rcParams['xtick.labelsize'] = 20\n",
    "plt.rcParams['ytick.labelsize'] = 20\n",
    "\n",
    "pd.options.display.max_columns = 1000\n",
    "\n",
    "DATA_PATH = '../cell-profiler/measurements'\n",
    "DATA_PATH2 = '../datasets/'\n",
    "SUFFIX = 'gain2_'\n",
    "CYTOPLASM = True\n",
    "ZERNIKE = True\n",
    "BIOMARKERS = True\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "DATA_PATH = '../cell-profiler/measurements'\n",
    "\n",
    "\n",
    "def load_data(filename, data_path=DATA_PATH):\n",
    "    \"\"\" \n",
    "    Read a csv file.\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(data_path, filename)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def save_data(df, filename, data_path=DATA_PATH):\n",
    "    \"\"\" \n",
    "    Write into a csv file.\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(data_path, filename)\n",
    "    df.to_csv(csv_path)\n",
    "    \n",
    "def parse_metadata(df):\n",
    "    \"\"\"\n",
    "    Parse metadata tp extract information about experimental conditions\n",
    "    and drop unnecessary columns.\n",
    "    \"\"\"\n",
    "    # Drop unnecessary columns\n",
    "    columns = ['Metadata_Frame', 'Metadata_Series',\n",
    "               'Metadata_Stiffness.1', 'Metadata_Combination.1', \n",
    "               'Metadata_Well.1', 'Metadata_Site.1'\n",
    "              ]\n",
    "    df.drop(columns, axis=1, inplace=True)\n",
    "    \n",
    "    # Rename columns containing metadata\n",
    "    df.rename(columns={'ImageNumber' : 'image', 'ObjectNumber' : 'object', \n",
    "                       'Metadata_Stiffness' : 'stiffness', \n",
    "                       'Metadata_Combination' : 'combination', \n",
    "                       'Metadata_Well' : 'well', \n",
    "                       'Metadata_Site' : 'site'}, inplace=True)\n",
    "    \n",
    "    # Change types and create cell and image labels \n",
    "    df = create_label(df)\n",
    "    df = create_label(df, col_name='image', per_cell=False)\n",
    "\n",
    "def as_stiff_type(x):\n",
    "    \"\"\"\n",
    "    Convert stiffness values to a custom categorical type.\n",
    "    \"\"\"\n",
    "    # Create a categorical data type for stiffness\n",
    "    stiff_type = CategoricalDtype(categories=['0.2', '0.5', '2.0', '8.0', '16.0', '32.0', '64.0'], ordered=True)\n",
    "    return x.astype(stiff_type)\n",
    "    \n",
    "    \n",
    "def create_label(df, col_name='label', per_cell=True):\n",
    "    \"\"\"\n",
    "    Create a unique label for each observation.\n",
    "    Labels are created as follows:\n",
    "    For each site: stiffness-combination-well-site\n",
    "    For each cell: stiffness-combination-well-site-object\n",
    "    \"\"\"\n",
    "    # Convert to non-numeric values\n",
    "    columns = ['image', 'object', 'stiffness', 'combination', 'well', 'site']\n",
    "    existing_columns = [col for col in columns if col in df.columns]\n",
    "    df[existing_columns] = df[existing_columns].astype(str)\n",
    "   \n",
    "    df.stiffness = as_stiff_type(df.stiffness)\n",
    "    \n",
    "    # Create a unique label for each cell\n",
    "    if per_cell:\n",
    "        df[col_name] = df[['stiffness', 'combination', 'well', 'site', 'object']].apply(lambda x: '-'.join(x), axis=1)\n",
    "    else:\n",
    "        df[col_name] = df[['stiffness', 'combination', 'well', 'site']].apply(lambda x: '-'.join(x), axis=1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    Rename columns containing features measured by cell profiler.\n",
    "    \"\"\"\n",
    "    # Convert to lower case\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    # Rename channels\n",
    "    df.columns = [col.replace('_origdapi', '_dapi', 1) for col in df.columns]\n",
    "    df.columns = [col.replace('_origwga', '_wga', 1) for col in df.columns]\n",
    "    df.columns = [col.replace('_origker', '_ker', 1) for col in df.columns]\n",
    "    df.columns = [col.replace('_origvim', '_vim', 1) for col in df.columns]\n",
    "    \n",
    "    # Coordinates in X\n",
    "    df.columns = [col.replace('_x', 'X', 1) for col in df.columns]\n",
    "\n",
    "    # Coordinates in Y\n",
    "    df.columns = [col.replace('_y', 'Y', 1) for col in df.columns]\n",
    "\n",
    "    # Coordinates in Z\n",
    "    df.columns = [col.replace('_z', 'Z', 1) for col in df.columns]\n",
    "\n",
    "    # Shape features\n",
    "    df.columns = [col.replace('areashape_', '', 1) for col in df.columns]\n",
    "    \n",
    "    # Zernike features\n",
    "    df.columns = [col.replace('areashapeZernike', 'zernike', 1) for col in df.columns]\n",
    "\n",
    "    # Intensity features\n",
    "    df.columns = [col.replace('intensity_', '', 1) for col in df.columns]\n",
    "\n",
    "    # Location\n",
    "    df.columns = [col.replace('location_', 'loc_', 1) for col in df.columns]\n",
    "\n",
    "    # Neighbours and Zernike\n",
    "    new_names = []\n",
    "    for col in df.columns:\n",
    "        if 'neighbors_' in col:\n",
    "            new_names.append(col.split('_')[1])\n",
    "        else:\n",
    "            new_names.append(col)\n",
    "    df.columns = new_names\n",
    "\n",
    "    # Texture\n",
    "    df.columns = [col.replace('_3', '', 1) for col in df.columns]\n",
    "    df.columns = [col.replace('texture_', '', 1) for col in df.columns]\n",
    "\n",
    "    print(\"The are no duplicated column names:\", len(list(df.columns)) == len(set(list(df.columns))))\n",
    "\n",
    "    \n",
    "def merge_datasets(df1, df2, suffixes=[]):\n",
    "    \"\"\"\n",
    "    Merge two datasets on a set of metedata columns.\n",
    "    \"\"\"\n",
    "    common_cols = ['label', 'image', 'object', 'stiffness', 'combination', 'well', 'site']\n",
    "    if len(suffixes)==2:\n",
    "        return pd.merge(df1, df2, how='outer', on=common_cols,  suffixes=suffixes)\n",
    "    elif len(suffixes)==1:\n",
    "        new_names=[]\n",
    "        for col in df2.columns:\n",
    "            if col in common_cols:\n",
    "                new_names.append(col)\n",
    "            else:\n",
    "                new_names.append(col + suffixes[0])\n",
    "        df2.columns = new_names\n",
    "        return pd.merge(df1, df2, how='outer', on=common_cols)\n",
    "    else:\n",
    "        return pd.merge(df1, df2, how='outer', on=common_cols)\n",
    "\n",
    "\n",
    "def move_column(df, column_name, loc):\n",
    "    \"\"\"\n",
    "    Move a columns in front of the dataframe.\n",
    "    \"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    columns.insert(loc, columns.pop(columns.index(column_name)))\n",
    "    return df.reindex(columns=columns, copy=False) \n",
    "\n",
    "    \n",
    "def split_dataset(df, channels=None):\n",
    "    \"\"\"\n",
    "    Split a dataset in two by channel name.\n",
    "    \"\"\"\n",
    "    subsets = []\n",
    "    \n",
    "    for selected in channels:\n",
    "        subset = df.copy()\n",
    "        \n",
    "        # Channels except for the selected\n",
    "        channels_to_drop = list(set(channels) - set([selected]))\n",
    "        \n",
    "        for channel in channels_to_drop:\n",
    "            # Drop the columns with specified channel\n",
    "            cols = [col for col in df.columns if channel in col]\n",
    "            subset.drop(cols, axis=1, inplace=True)\n",
    "            \n",
    "        # Rename channel suffix in the new dataset\n",
    "        subset.columns = [col.replace('_' + selected, '', 1) for col in subset.columns]\n",
    "        subsets.append(subset)\n",
    "        \n",
    "    return subsets\n",
    "    \n",
    "\n",
    "def dist(df):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance on a dataframe.\n",
    "    Input columns are arranged as x0, x1, y0, y1.\n",
    "    \"\"\"\n",
    "    return np.sqrt((df.iloc[:,0] - df.iloc[:,2])**2 + (df.iloc[:,1] - df.iloc[:,3])**2)\n",
    "\n",
    "def select_features(df, filename='selected_columns.txt'):\n",
    "    \"\"\"\n",
    "    Load the list of manually selected columns\n",
    "    and return a copy of the dataset containing only\n",
    "    those columns\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        selected_cols = [line.rstrip('\\n') for line in file]\n",
    "    \n",
    "    df_fs = pd.concat([df.loc[:, 'label' : 'well'], df[selected_cols]], axis=1)\n",
    "    \n",
    "    return df_fs\n",
    "    \n",
    "    \n",
    "def undersample(df, n_samples):\n",
    "    \"\"\"\n",
    "    Perform undersampling of majority classes\n",
    "    by randomly selecting n_samples cells \n",
    "    for each stiffness level.\n",
    "    \"\"\"\n",
    "    df_under = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    for s in df.stiffness.unique():\n",
    "        if (s == \"8.0\") or (s == \"32.0\") :\n",
    "            df_under = pd.concat([df_under, df[df.stiffness == s]], axis=0)\n",
    "        else:\n",
    "            df_under = pd.concat([df_under, df[df.stiffness == s].sample(n_samples)], axis=0)\n",
    "        \n",
    "    print(\"Undersampling. The balanced dataset has shape\", df_under.shape)\n",
    "    return df_under.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def smote(X, y, as_df=True):   \n",
    "    \"\"\"\n",
    "    Synthesise new observations to have equal \n",
    "    number of cells for each stiffness value.\n",
    "    \"\"\"\n",
    "    smote = SMOTE()\n",
    "    X_sm, y_sm = smote.fit_sample(X, y)\n",
    "    print(\"\\nAfter synthesing new observations the balanced dataset has {} rows and {} columns.\\n\"\n",
    "          .format(X_sm.shape[0], X_sm.shape[1]))\n",
    "    \n",
    "    if as_df:\n",
    "        df_smote = pd.concat([pd.DataFrame(X_sm, columns=X.columns), \n",
    "                              as_stiff_type(pd.DataFrame(y_sm, columns=['stiffness']))],\n",
    "                             axis=1).sort_values(by='stiffness')\n",
    "        return df_smote\n",
    "    else:\n",
    "        return X_sm, y_sm\n",
    "    \n",
    "def cv_ratio(df, col1='ctcf_ker', col2='ctcf_vim'):\n",
    "    df['cvratio'] = df[col1] / df[col2]\n",
    "    df['log_cvratio'] = np.log(df.cvratio)\n",
    "    return df\n",
    "\n",
    "def nc_ratio(df):\n",
    "    df['ncr'] = df.area_nucl / df.area_cyto\n",
    "    return df\n",
    "\n",
    "def phenotype(df, use, cols=('meanintensity_ker', 'meanintensity_vim')):\n",
    "    if use=='log_ratio':\n",
    "        df['log_ratio'] = np.log(df[cols[0]]/df[cols[1]])\n",
    "        q1 = df.log_ratio.quantile(0.33)\n",
    "        q3 = df.log_ratio.quantile(0.66)\n",
    "        df['region'] = pd.cut(df.log_ratio, \n",
    "                             bins=(df.log_ratio.min(), q1, q3, df.log_ratio.max()), \n",
    "                             labels=[\"low\", \"med\", \"high\"], include_lowest=True)\n",
    "    elif use=='log_biom':\n",
    "        df['log_ker'] = np.log(df[cols[0]])\n",
    "        df['log_vim'] = np.log(df[cols[1]])\n",
    "        df['region'] = \"low\"\n",
    "        df.loc[(df.log_ker < df.log_ker.median()) &\n",
    "                         (df.log_vim > df.log_vim.median()), 'region'] = \"high vim\"\n",
    "        df.loc[(df.log_ker > df.log_ker.median()) &\n",
    "                         (df.log_vim < df.log_vim.median()), 'region'] = \"high ker\"\n",
    "        df.loc[(df.log_ker > df.log_ker.median()) &\n",
    "                         (df.log_vim > df.log_vim.median()), 'region'] = \"high\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_neighbors(df, df_n):\n",
    "    cols = ['ImageNumber', 'Location_Center_X', 'Location_Center_Y']\n",
    "    \n",
    "    # Round centre locations\n",
    "    df_m = df[cols].copy()\n",
    "    loc_cols = ['Location_Center_X', 'Location_Center_Y']\n",
    "    df_m[loc_cols] = df_m[loc_cols].round()\n",
    "    df_n[loc_cols] = df_n[loc_cols].round()\n",
    "    \n",
    "    # Merge dataframes\n",
    "    neighb = pd.merge(df_m, df_n, how='inner', on=cols)\n",
    "    \n",
    "    # Assign values to the original dataframe\n",
    "    df.loc[:, 'Neighbors_AngleBetweenNeighbors_3' : 'Number_Object_Number'] = \\\n",
    "    neighb.loc[:, 'Neighbors_AngleBetweenNeighbors_3' : 'Number_Object_Number']\n",
    "    \n",
    "    # Delete duplicated columns\n",
    "    distances = [col.split('_')[2] for col in neighb.columns if 'Neighbors_NumberOfNeighbors' in col]\n",
    "\n",
    "    if len(distances) > 1:\n",
    "        dupl_cols = ['Neighbors_AngleBetweenNeighbors_', \n",
    "                     'Neighbors_FirstClosestDistance_', \n",
    "                     'Neighbors_FirstClosestObjectNumber_', \n",
    "                     'Neighbors_SecondClosestDistance_',\n",
    "                     'Neighbors_SecondClosestObjectNumber_']\n",
    "        dupl_cols = [col + distances[1] for col in dupl_cols]\n",
    "        df.drop(dupl_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    Rename columns containing features measured by cell profiler.\n",
    "    \"\"\"\n",
    "    # Convert to lower case\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    # Rename channels\n",
    "    df.columns = [col.replace('_origdapi', '_dapi', 1) for col in df.columns]\n",
    "    df.columns = [col.replace('_origwga', '_wga', 1) for col in df.columns]\n",
    "    df.columns = [col.replace('_origker', '_ker', 1) for col in df.columns]\n",
    "    df.columns = [col.replace('_origvim', '_vim', 1) for col in df.columns]\n",
    "    \n",
    "    # Coordinates in X\n",
    "    df.columns = [col.replace('_x', 'X', 1) for col in df.columns]\n",
    "\n",
    "    # Coordinates in Y\n",
    "    df.columns = [col.replace('_y', 'Y', 1) for col in df.columns]\n",
    "\n",
    "    # Coordinates in Z\n",
    "    df.columns = [col.replace('_z', 'Z', 1) for col in df.columns]\n",
    "\n",
    "    # Shape features\n",
    "    df.columns = [col.replace('areashape_', '', 1) for col in df.columns]\n",
    "    \n",
    "    # Zernike features\n",
    "    df.columns = [col.replace('areashapeZernike', 'zernike', 1) for col in df.columns]\n",
    "\n",
    "    # Intensity features\n",
    "    df.columns = [col.replace('intensity_', '', 1) for col in df.columns]\n",
    "\n",
    "    # Location\n",
    "    df.columns = [col.replace('location_', 'loc_', 1) for col in df.columns]\n",
    "\n",
    "    # Texture\n",
    "    new_names = []\n",
    "    for col in df.columns:\n",
    "        if 'texture_' in col:\n",
    "            new_names.append(col.replace('_3', '', 1))\n",
    "        else:\n",
    "            new_names.append(col)\n",
    "    df.columns = new_names\n",
    "    df.columns = [col.replace('texture_', '', 1) for col in df.columns]\n",
    "\n",
    "    print(\"The are no duplicated column names:\", len(list(df.columns)) == len(set(list(df.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_cell_data(data_path=DATA_PATH, suffix='', cytoplasm=False, biomarkers=False):\n",
    "    \"\"\" \n",
    "    Import all the data and then \n",
    "    call functions to parse metadata, \n",
    "    rename and rearrange columns and merge datasets.\n",
    "    \"\"\"\n",
    "    cells = load_data(filename=suffix + 'Cells.csv')\n",
    "    neighbors = load_data(filename=suffix + 'Neighbours.csv')\n",
    "    nuclei = load_data(filename=suffix + 'Nuclei.csv')\n",
    "    info = load_data(filename=suffix + 'Image.csv')\n",
    "    \n",
    "    # Check that dataframes contain the correct number of cells \n",
    "    n_cells = info.Count_Cells.sum()\n",
    "    print('Morphology was measured for {} cells.\\n'.format(n_cells))\n",
    "    \n",
    "    if (cells.shape[0] == n_cells) and (nuclei.shape[0] == n_cells):\n",
    "        print('The numbers of cells and nuclei correspond to each other.\\n')\n",
    "    else:\n",
    "        print('Found {} cells and {} nuclei'.format(cells.shape[0], nuclei.shape[0]))\n",
    "        \n",
    "    # Merge neighbours\n",
    "    merge_neighbors(cells, neighbors)\n",
    "    \n",
    "    # Parse and clean metadata\n",
    "    parse_metadata(cells)\n",
    "    parse_metadata(nuclei)\n",
    "\n",
    "    # Rename columns\n",
    "    rename_columns(cells)\n",
    "    rename_columns(nuclei)\n",
    "    \n",
    "    # Merge two datasets\n",
    "    measurements = merge_datasets(cells, nuclei, suffixes=['_cell', '_nucl'])   \n",
    "    \n",
    "    if cytoplasm == True:\n",
    "        cytoplasm = load_data(filename=suffix + 'Cytoplasm.csv') \n",
    "        print('Cytoplasm measurements were taken for {} cells.\\n'.format(cytoplasm.shape[0]))\n",
    "                  \n",
    "        # Parse and clean metadata       \n",
    "        parse_metadata(cytoplasm)\n",
    "        \n",
    "        # Rename columns\n",
    "        rename_columns(cytoplasm)\n",
    "        \n",
    "        # Merge with the main dataset\n",
    "        measurements = merge_datasets(measurements, cytoplasm, suffixes=['_cyto']) \n",
    "        \n",
    "    if biomarkers:\n",
    "        #biomarkers = load_data(filename=suffix + 'Biomarkers_Cells.csv')\n",
    "        print('Read biomarkers normalised to min gain')\n",
    "        biomarkers = load_data(filename='gain_Biomarkers_Cells.csv')\n",
    "        print('Biomarkers were measured for {} cells.\\n'.format(biomarkers.shape[0]))\n",
    "        \n",
    "        # Parse and clean metadata\n",
    "        parse_metadata(biomarkers)\n",
    "        \n",
    "        # Rename columns\n",
    "        rename_columns(biomarkers)\n",
    "        \n",
    "        # Merge with the main dataset\n",
    "        measurements = merge_datasets(measurements, biomarkers) \n",
    "    \n",
    "    # Move \"label\" column to front\n",
    "    measurements = move_column(measurements, 'label', 0)\n",
    "    print(\"\\nFull dataset has shape:\", measurements.shape)\n",
    "    \n",
    "    return measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean the dataframe by dropping variable with zero variance, \n",
    "    uninformative/duplicated columns and location measurements.\n",
    "    \"\"\"\n",
    "    # Check if there are any missing values\n",
    "    assert df.isnull().sum().sum() == 0\n",
    "    # measurements[measurements.isnull().sum(axis=1) > 0]\n",
    "    \n",
    "    print(\"Initial shape is:\", df.shape)\n",
    "    \n",
    "    # Calculate summary statistics and drop features with zero variance\n",
    "    stats = df.describe()\n",
    "    zerovar_cols = stats.columns[stats.loc['std', :] == 0]\n",
    "    print(\"Features with zero variance:\\n\", zerovar_cols)\n",
    "    df.drop(zerovar_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Drop columns with object numbers\n",
    "    numbers_cols = [col for col in df.columns if 'object' in col and 'number' in col]\n",
    "    df.drop(numbers_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Drop columns with parent numbers\n",
    "    parent_cols = [col for col in df.columns if 'parent' in col or 'children' in col]\n",
    "    df.drop(parent_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Transform location measurements\n",
    "    df = transform_location(df)\n",
    "    \n",
    "    # Transform orientation angle from [-pi/2, pi/2] to [0, pi]\n",
    "    angle_cols = [col for col in df.columns if 'orientation' in col]\n",
    "    df[angle_cols] += 90\n",
    "    \n",
    "    print(\"\\nAfter cleaning the dataset has {} rows and {} columns.\\n\".format(df.shape[0], df.shape[1]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_location(df):\n",
    "    \"\"\"\n",
    "    Calculate the following distances and drop columns with location measurements.\n",
    "    * centerX, centerY are coordinates of the fartherst point from any edge \n",
    "    (calculated using scipy.ndimage.center_of_mass);\n",
    "    * loc_centerX, loc_centerY are average coordinates for the binary image\n",
    "    (calculated using scipy.ndimage.mean);\n",
    "    * fartherstpoint is a distance between the two points\n",
    "    relative to cell and nuclear boundaries, respectively;\n",
    "    * loc_maxintensityX, loc_maxintensityY are coordinates of the pixel \n",
    "    with the maximum intensity within the object;\n",
    "    * maxintdisplacement is a distance between this pixel and loc_center;\n",
    "    * nucleusshift is a distances between centres of mass of a cell and its nucleus.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop duplicate columns with location\n",
    "    df.drop(['loc_centerX', 'loc_centerY'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Drop \"centermassintensity\" columns\n",
    "    drop_cols = [col for col in df.columns if 'centermassintensity' in col]\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "   \n",
    "    # Calculate distances between centres of a binary image\n",
    "    df['fartherstpoint_cell'] = dist(df.loc[:, ['centerX_cell', \n",
    "                                                'centerY_cell',\n",
    "                                                'loc_centerX_cell', \n",
    "                                                'loc_centerY_cell']])\n",
    "    df['fartherstpoint_nucl'] = dist(df.loc[:, ['centerX_nucl', \n",
    "                                                'centerY_nucl',\n",
    "                                                'loc_centerX_nucl', \n",
    "                                                'loc_centerY_nucl']])\n",
    "    df['nucleusshift'] = dist(df.loc[:, ['centerX_cell', \n",
    "                                         'centerY_cell', \n",
    "                                         'centerX_nucl', \n",
    "                                         'centerY_nucl']])\n",
    "    \n",
    "    # Calculate max intensity displacement\n",
    "    suffix = ['_'.join(col.split('_')[2:]) for col in df.columns if 'loc_maxintensity' in col]\n",
    "    for s in set(suffix):\n",
    "        maxint_cols = [col for col in df.columns if 'loc_maxintensity' in col and s in col]\n",
    "        if 'dapi' in s or 'nucl' in s:\n",
    "            cols = ['loc_centerX_nucl','loc_centerY_nucl']\n",
    "            cols.extend(maxint_cols)\n",
    "        else:\n",
    "            cols = ['loc_centerX_cell','loc_centerY_cell']\n",
    "            cols.extend(maxint_cols)\n",
    "\n",
    "        new_col = 'maxintdisplacement_' + s\n",
    "        df[new_col] = dist(df.loc[:, cols])\n",
    "    \n",
    "    # All location measurements are in absolute coordinates and should be dropped\n",
    "    drop_cols = [col for col in df.columns if 'loc' in col]\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "    drop_cols = [col for col in df.columns if 'center' in col]\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Move mass displacement columns to the end\n",
    "    mass_cols = [col for col in df.columns if 'mass' in col]\n",
    "    for col in mass_cols:\n",
    "        df = move_column(df, col, df.columns.size)\n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morphology was measured for 610.0 cells.\n",
      "\n",
      "The numbers of cells and nuclei correspond to each other.\n",
      "\n",
      "The are no duplicated column names: True\n",
      "The are no duplicated column names: True\n",
      "Cytoplasm measurements were taken for 610 cells.\n",
      "\n",
      "The are no duplicated column names: True\n",
      "Read biomarkers normalised to min gain\n",
      "Biomarkers were measured for 610 cells.\n",
      "\n",
      "The are no duplicated column names: True\n",
      "\n",
      "Full dataset has shape: (610, 484)\n"
     ]
    }
   ],
   "source": [
    "df = import_cell_data(data_path=DATA_PATH, suffix=SUFFIX, cytoplasm=CYTOPLASM, biomarkers=BIOMARKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape is: (610, 484)\n",
      "Features with zero variance:\n",
      " Index(['centerZ_cell', 'eulernumber_cell', 'children_cytoplasm_count_cell',\n",
      "       'loc_centermassintensityZ_wga', 'loc_centerZ_cell',\n",
      "       'loc_maxintensityZ_wga', 'centerZ_nucl', 'eulernumber_nucl',\n",
      "       'children_cells_count', 'children_cytoplasm_count_nucl',\n",
      "       'loc_centermassintensityZ_dapi', 'loc_centerZ_nucl',\n",
      "       'loc_maxintensityZ_dapi', 'centerZ_cyto',\n",
      "       'loc_centermassintensityZ_ker', 'loc_centermassintensityZ_vim',\n",
      "       'loc_centerZ', 'loc_maxintensityZ_ker', 'loc_maxintensityZ_vim'],\n",
      "      dtype='object')\n",
      "\n",
      "After cleaning the dataset has 610 rows and 430 columns.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'image',\n",
       " 'object',\n",
       " 'combination',\n",
       " 'site',\n",
       " 'stiffness',\n",
       " 'well',\n",
       " 'area_cell',\n",
       " 'compactness_cell',\n",
       " 'eccentricity_cell',\n",
       " 'extent_cell',\n",
       " 'formfactor_cell',\n",
       " 'majoraxislength_cell',\n",
       " 'maxferetdiameter_cell',\n",
       " 'maximumradius_cell',\n",
       " 'meanradius_cell',\n",
       " 'medianradius_cell',\n",
       " 'minferetdiameter_cell',\n",
       " 'minoraxislength_cell',\n",
       " 'orientation_cell',\n",
       " 'perimeter_cell',\n",
       " 'solidity_cell',\n",
       " 'zernike_0_0_cell',\n",
       " 'zernike_1_1_cell',\n",
       " 'zernike_2_0_cell',\n",
       " 'zernike_2_2_cell',\n",
       " 'zernike_3_1_cell',\n",
       " 'zernike_3_3_cell',\n",
       " 'zernike_4_0_cell',\n",
       " 'zernike_4_2_cell',\n",
       " 'zernike_4_4_cell',\n",
       " 'zernike_5_1_cell',\n",
       " 'zernike_5_3_cell',\n",
       " 'zernike_5_5_cell',\n",
       " 'zernike_6_0_cell',\n",
       " 'zernike_6_2_cell',\n",
       " 'zernike_6_4_cell',\n",
       " 'zernike_6_6_cell',\n",
       " 'zernike_7_1_cell',\n",
       " 'zernike_7_3_cell',\n",
       " 'zernike_7_5_cell',\n",
       " 'zernike_7_7_cell',\n",
       " 'zernike_8_0_cell',\n",
       " 'zernike_8_2_cell',\n",
       " 'zernike_8_4_cell',\n",
       " 'zernike_8_6_cell',\n",
       " 'zernike_8_8_cell',\n",
       " 'zernike_9_1_cell',\n",
       " 'zernike_9_3_cell',\n",
       " 'zernike_9_5_cell',\n",
       " 'zernike_9_7_cell',\n",
       " 'zernike_9_9_cell',\n",
       " 'integratedintensityedge_wga',\n",
       " 'integratedintensity_wga',\n",
       " 'lowerquartileintensity_wga',\n",
       " 'madintensity_wga',\n",
       " 'maxintensityedge_wga',\n",
       " 'maxintensity_wga',\n",
       " 'meanintensityedge_wga',\n",
       " 'meanintensity_wga',\n",
       " 'medianintensity_wga',\n",
       " 'minintensityedge_wga',\n",
       " 'minintensity_wga',\n",
       " 'stdintensityedge_wga',\n",
       " 'stdintensity_wga',\n",
       " 'upperquartileintensity_wga',\n",
       " 'neighbors_anglebetweenneighbors_3',\n",
       " 'neighbors_firstclosestdistance_3',\n",
       " 'neighbors_numberofneighbors_3',\n",
       " 'neighbors_numberofneighbors_30',\n",
       " 'neighbors_percenttouching_3',\n",
       " 'neighbors_percenttouching_30',\n",
       " 'neighbors_secondclosestdistance_3',\n",
       " 'angularsecondmoment_wga_00',\n",
       " 'angularsecondmoment_wga_01',\n",
       " 'angularsecondmoment_wga_02',\n",
       " 'angularsecondmoment_wga_03',\n",
       " 'contrast_wga_00',\n",
       " 'contrast_wga_01',\n",
       " 'contrast_wga_02',\n",
       " 'contrast_wga_03',\n",
       " 'correlation_wga_00',\n",
       " 'correlation_wga_01',\n",
       " 'correlation_wga_02',\n",
       " 'correlation_wga_03',\n",
       " 'differenceentropy_wga_00',\n",
       " 'differenceentropy_wga_01',\n",
       " 'differenceentropy_wga_02',\n",
       " 'differenceentropy_wga_03',\n",
       " 'differencevariance_wga_00',\n",
       " 'differencevariance_wga_01',\n",
       " 'differencevariance_wga_02',\n",
       " 'differencevariance_wga_03',\n",
       " 'entropy_wga_00',\n",
       " 'entropy_wga_01',\n",
       " 'entropy_wga_02',\n",
       " 'entropy_wga_03',\n",
       " 'infomeas1_wga_00',\n",
       " 'infomeas1_wga_01',\n",
       " 'infomeas1_wga_02',\n",
       " 'infomeas1_wga_03',\n",
       " 'infomeas2_wga_00',\n",
       " 'infomeas2_wga_01',\n",
       " 'infomeas2_wga_02',\n",
       " 'infomeas2_wga_03',\n",
       " 'inversedifferencemoment_wga_00',\n",
       " 'inversedifferencemoment_wga_01',\n",
       " 'inversedifferencemoment_wga_02',\n",
       " 'inversedifferencemoment_wga_03',\n",
       " 'sumaverage_wga_00',\n",
       " 'sumaverage_wga_01',\n",
       " 'sumaverage_wga_02',\n",
       " 'sumaverage_wga_03',\n",
       " 'sumentropy_wga_00',\n",
       " 'sumentropy_wga_01',\n",
       " 'sumentropy_wga_02',\n",
       " 'sumentropy_wga_03',\n",
       " 'sumvariance_wga_00',\n",
       " 'sumvariance_wga_01',\n",
       " 'sumvariance_wga_02',\n",
       " 'sumvariance_wga_03',\n",
       " 'variance_wga_00',\n",
       " 'variance_wga_01',\n",
       " 'variance_wga_02',\n",
       " 'variance_wga_03',\n",
       " 'area_nucl',\n",
       " 'compactness_nucl',\n",
       " 'eccentricity_nucl',\n",
       " 'extent_nucl',\n",
       " 'formfactor_nucl',\n",
       " 'majoraxislength_nucl',\n",
       " 'maxferetdiameter_nucl',\n",
       " 'maximumradius_nucl',\n",
       " 'meanradius_nucl',\n",
       " 'medianradius_nucl',\n",
       " 'minferetdiameter_nucl',\n",
       " 'minoraxislength_nucl',\n",
       " 'orientation_nucl',\n",
       " 'perimeter_nucl',\n",
       " 'solidity_nucl',\n",
       " 'zernike_0_0_nucl',\n",
       " 'zernike_1_1_nucl',\n",
       " 'zernike_2_0_nucl',\n",
       " 'zernike_2_2_nucl',\n",
       " 'zernike_3_1_nucl',\n",
       " 'zernike_3_3_nucl',\n",
       " 'zernike_4_0_nucl',\n",
       " 'zernike_4_2_nucl',\n",
       " 'zernike_4_4_nucl',\n",
       " 'zernike_5_1_nucl',\n",
       " 'zernike_5_3_nucl',\n",
       " 'zernike_5_5_nucl',\n",
       " 'zernike_6_0_nucl',\n",
       " 'zernike_6_2_nucl',\n",
       " 'zernike_6_4_nucl',\n",
       " 'zernike_6_6_nucl',\n",
       " 'zernike_7_1_nucl',\n",
       " 'zernike_7_3_nucl',\n",
       " 'zernike_7_5_nucl',\n",
       " 'zernike_7_7_nucl',\n",
       " 'zernike_8_0_nucl',\n",
       " 'zernike_8_2_nucl',\n",
       " 'zernike_8_4_nucl',\n",
       " 'zernike_8_6_nucl',\n",
       " 'zernike_8_8_nucl',\n",
       " 'zernike_9_1_nucl',\n",
       " 'zernike_9_3_nucl',\n",
       " 'zernike_9_5_nucl',\n",
       " 'zernike_9_7_nucl',\n",
       " 'zernike_9_9_nucl',\n",
       " 'integratedintensityedge_dapi',\n",
       " 'integratedintensity_dapi',\n",
       " 'lowerquartileintensity_dapi',\n",
       " 'madintensity_dapi',\n",
       " 'maxintensityedge_dapi',\n",
       " 'maxintensity_dapi',\n",
       " 'meanintensityedge_dapi',\n",
       " 'meanintensity_dapi',\n",
       " 'medianintensity_dapi',\n",
       " 'minintensityedge_dapi',\n",
       " 'minintensity_dapi',\n",
       " 'stdintensityedge_dapi',\n",
       " 'stdintensity_dapi',\n",
       " 'upperquartileintensity_dapi',\n",
       " 'neighbors_anglebetweenneighbors_20',\n",
       " 'neighbors_firstclosestdistance_20',\n",
       " 'neighbors_numberofneighbors_20',\n",
       " 'neighbors_percenttouching_20',\n",
       " 'neighbors_secondclosestdistance_20',\n",
       " 'angularsecondmoment_dapi_00',\n",
       " 'angularsecondmoment_dapi_01',\n",
       " 'angularsecondmoment_dapi_02',\n",
       " 'angularsecondmoment_dapi_03',\n",
       " 'contrast_dapi_00',\n",
       " 'contrast_dapi_01',\n",
       " 'contrast_dapi_02',\n",
       " 'contrast_dapi_03',\n",
       " 'correlation_dapi_00',\n",
       " 'correlation_dapi_01',\n",
       " 'correlation_dapi_02',\n",
       " 'correlation_dapi_03',\n",
       " 'differenceentropy_dapi_00',\n",
       " 'differenceentropy_dapi_01',\n",
       " 'differenceentropy_dapi_02',\n",
       " 'differenceentropy_dapi_03',\n",
       " 'differencevariance_dapi_00',\n",
       " 'differencevariance_dapi_01',\n",
       " 'differencevariance_dapi_02',\n",
       " 'differencevariance_dapi_03',\n",
       " 'entropy_dapi_00',\n",
       " 'entropy_dapi_01',\n",
       " 'entropy_dapi_02',\n",
       " 'entropy_dapi_03',\n",
       " 'infomeas1_dapi_00',\n",
       " 'infomeas1_dapi_01',\n",
       " 'infomeas1_dapi_02',\n",
       " 'infomeas1_dapi_03',\n",
       " 'infomeas2_dapi_00',\n",
       " 'infomeas2_dapi_01',\n",
       " 'infomeas2_dapi_02',\n",
       " 'infomeas2_dapi_03',\n",
       " 'inversedifferencemoment_dapi_00',\n",
       " 'inversedifferencemoment_dapi_01',\n",
       " 'inversedifferencemoment_dapi_02',\n",
       " 'inversedifferencemoment_dapi_03',\n",
       " 'sumaverage_dapi_00',\n",
       " 'sumaverage_dapi_01',\n",
       " 'sumaverage_dapi_02',\n",
       " 'sumaverage_dapi_03',\n",
       " 'sumentropy_dapi_00',\n",
       " 'sumentropy_dapi_01',\n",
       " 'sumentropy_dapi_02',\n",
       " 'sumentropy_dapi_03',\n",
       " 'sumvariance_dapi_00',\n",
       " 'sumvariance_dapi_01',\n",
       " 'sumvariance_dapi_02',\n",
       " 'sumvariance_dapi_03',\n",
       " 'variance_dapi_00',\n",
       " 'variance_dapi_01',\n",
       " 'variance_dapi_02',\n",
       " 'variance_dapi_03',\n",
       " 'area_cyto',\n",
       " 'compactness_cyto',\n",
       " 'eccentricity_cyto',\n",
       " 'eulernumber_cyto',\n",
       " 'extent_cyto',\n",
       " 'formfactor_cyto',\n",
       " 'majoraxislength_cyto',\n",
       " 'maxferetdiameter_cyto',\n",
       " 'maximumradius_cyto',\n",
       " 'meanradius_cyto',\n",
       " 'medianradius_cyto',\n",
       " 'minferetdiameter_cyto',\n",
       " 'minoraxislength_cyto',\n",
       " 'orientation_cyto',\n",
       " 'perimeter_cyto',\n",
       " 'solidity_cyto',\n",
       " 'zernike_0_0_cyto',\n",
       " 'zernike_1_1_cyto',\n",
       " 'zernike_2_0_cyto',\n",
       " 'zernike_2_2_cyto',\n",
       " 'zernike_3_1_cyto',\n",
       " 'zernike_3_3_cyto',\n",
       " 'zernike_4_0_cyto',\n",
       " 'zernike_4_2_cyto',\n",
       " 'zernike_4_4_cyto',\n",
       " 'zernike_5_1_cyto',\n",
       " 'zernike_5_3_cyto',\n",
       " 'zernike_5_5_cyto',\n",
       " 'zernike_6_0_cyto',\n",
       " 'zernike_6_2_cyto',\n",
       " 'zernike_6_4_cyto',\n",
       " 'zernike_6_6_cyto',\n",
       " 'zernike_7_1_cyto',\n",
       " 'zernike_7_3_cyto',\n",
       " 'zernike_7_5_cyto',\n",
       " 'zernike_7_7_cyto',\n",
       " 'zernike_8_0_cyto',\n",
       " 'zernike_8_2_cyto',\n",
       " 'zernike_8_4_cyto',\n",
       " 'zernike_8_6_cyto',\n",
       " 'zernike_8_8_cyto',\n",
       " 'zernike_9_1_cyto',\n",
       " 'zernike_9_3_cyto',\n",
       " 'zernike_9_5_cyto',\n",
       " 'zernike_9_7_cyto',\n",
       " 'zernike_9_9_cyto',\n",
       " 'integratedintensityedge_ker',\n",
       " 'integratedintensityedge_vim',\n",
       " 'integratedintensity_ker',\n",
       " 'integratedintensity_vim',\n",
       " 'lowerquartileintensity_ker',\n",
       " 'lowerquartileintensity_vim',\n",
       " 'madintensity_ker',\n",
       " 'madintensity_vim',\n",
       " 'maxintensityedge_ker',\n",
       " 'maxintensityedge_vim',\n",
       " 'maxintensity_ker',\n",
       " 'maxintensity_vim',\n",
       " 'meanintensityedge_ker',\n",
       " 'meanintensityedge_vim',\n",
       " 'meanintensity_ker',\n",
       " 'meanintensity_vim',\n",
       " 'medianintensity_ker',\n",
       " 'medianintensity_vim',\n",
       " 'minintensityedge_ker',\n",
       " 'minintensityedge_vim',\n",
       " 'minintensity_ker',\n",
       " 'minintensity_vim',\n",
       " 'stdintensityedge_ker',\n",
       " 'stdintensityedge_vim',\n",
       " 'stdintensity_ker',\n",
       " 'stdintensity_vim',\n",
       " 'upperquartileintensity_ker',\n",
       " 'upperquartileintensity_vim',\n",
       " 'angularsecondmoment_ker_00',\n",
       " 'angularsecondmoment_ker_01',\n",
       " 'angularsecondmoment_ker_02',\n",
       " 'angularsecondmoment_ker_03',\n",
       " 'angularsecondmoment_vim_00',\n",
       " 'angularsecondmoment_vim_01',\n",
       " 'angularsecondmoment_vim_02',\n",
       " 'angularsecondmoment_vim_03',\n",
       " 'contrast_ker_00',\n",
       " 'contrast_ker_01',\n",
       " 'contrast_ker_02',\n",
       " 'contrast_ker_03',\n",
       " 'contrast_vim_00',\n",
       " 'contrast_vim_01',\n",
       " 'contrast_vim_02',\n",
       " 'contrast_vim_03',\n",
       " 'correlation_ker_00',\n",
       " 'correlation_ker_01',\n",
       " 'correlation_ker_02',\n",
       " 'correlation_ker_03',\n",
       " 'correlation_vim_00',\n",
       " 'correlation_vim_01',\n",
       " 'correlation_vim_02',\n",
       " 'correlation_vim_03',\n",
       " 'differenceentropy_ker_00',\n",
       " 'differenceentropy_ker_01',\n",
       " 'differenceentropy_ker_02',\n",
       " 'differenceentropy_ker_03',\n",
       " 'differenceentropy_vim_00',\n",
       " 'differenceentropy_vim_01',\n",
       " 'differenceentropy_vim_02',\n",
       " 'differenceentropy_vim_03',\n",
       " 'differencevariance_ker_00',\n",
       " 'differencevariance_ker_01',\n",
       " 'differencevariance_ker_02',\n",
       " 'differencevariance_ker_03',\n",
       " 'differencevariance_vim_00',\n",
       " 'differencevariance_vim_01',\n",
       " 'differencevariance_vim_02',\n",
       " 'differencevariance_vim_03',\n",
       " 'entropy_ker_00',\n",
       " 'entropy_ker_01',\n",
       " 'entropy_ker_02',\n",
       " 'entropy_ker_03',\n",
       " 'entropy_vim_00',\n",
       " 'entropy_vim_01',\n",
       " 'entropy_vim_02',\n",
       " 'entropy_vim_03',\n",
       " 'infomeas1_ker_00',\n",
       " 'infomeas1_ker_01',\n",
       " 'infomeas1_ker_02',\n",
       " 'infomeas1_ker_03',\n",
       " 'infomeas1_vim_00',\n",
       " 'infomeas1_vim_01',\n",
       " 'infomeas1_vim_02',\n",
       " 'infomeas1_vim_03',\n",
       " 'infomeas2_ker_00',\n",
       " 'infomeas2_ker_01',\n",
       " 'infomeas2_ker_02',\n",
       " 'infomeas2_ker_03',\n",
       " 'infomeas2_vim_00',\n",
       " 'infomeas2_vim_01',\n",
       " 'infomeas2_vim_02',\n",
       " 'infomeas2_vim_03',\n",
       " 'inversedifferencemoment_ker_00',\n",
       " 'inversedifferencemoment_ker_01',\n",
       " 'inversedifferencemoment_ker_02',\n",
       " 'inversedifferencemoment_ker_03',\n",
       " 'inversedifferencemoment_vim_00',\n",
       " 'inversedifferencemoment_vim_01',\n",
       " 'inversedifferencemoment_vim_02',\n",
       " 'inversedifferencemoment_vim_03',\n",
       " 'sumaverage_ker_00',\n",
       " 'sumaverage_ker_01',\n",
       " 'sumaverage_ker_02',\n",
       " 'sumaverage_ker_03',\n",
       " 'sumaverage_vim_00',\n",
       " 'sumaverage_vim_01',\n",
       " 'sumaverage_vim_02',\n",
       " 'sumaverage_vim_03',\n",
       " 'sumentropy_ker_00',\n",
       " 'sumentropy_ker_01',\n",
       " 'sumentropy_ker_02',\n",
       " 'sumentropy_ker_03',\n",
       " 'sumentropy_vim_00',\n",
       " 'sumentropy_vim_01',\n",
       " 'sumentropy_vim_02',\n",
       " 'sumentropy_vim_03',\n",
       " 'sumvariance_ker_00',\n",
       " 'sumvariance_ker_01',\n",
       " 'sumvariance_ker_02',\n",
       " 'sumvariance_ker_03',\n",
       " 'sumvariance_vim_00',\n",
       " 'sumvariance_vim_01',\n",
       " 'sumvariance_vim_02',\n",
       " 'sumvariance_vim_03',\n",
       " 'variance_ker_00',\n",
       " 'variance_ker_01',\n",
       " 'variance_ker_02',\n",
       " 'variance_ker_03',\n",
       " 'variance_vim_00',\n",
       " 'variance_vim_01',\n",
       " 'variance_vim_02',\n",
       " 'variance_vim_03',\n",
       " 'fartherstpoint_cell',\n",
       " 'fartherstpoint_nucl',\n",
       " 'nucleusshift',\n",
       " 'maxintdisplacement_ker',\n",
       " 'maxintdisplacement_wga',\n",
       " 'maxintdisplacement_vim',\n",
       " 'maxintdisplacement_dapi',\n",
       " 'massdisplacement_wga',\n",
       " 'massdisplacement_dapi',\n",
       " 'massdisplacement_ker',\n",
       " 'massdisplacement_vim']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
